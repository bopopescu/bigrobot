* Settings
Documentation    T5 upgrade Test Suite
Suite Setup      Upgrade Suite Setup
Suite Teardown   Upgrade suite teardown
Test Setup       Upgrade base test setup
Test Teardown    Upgrade base test teardown
Force Tags       T5   Ironhorse   single_node   upgrade
Library			 OperatingSystem
Library          keywords/BsnCommon.py
Library			 keywords/Mininet.py
Library		     keywords/Host.py
Library		     keywords/Ixia.py
Library		     keywords_dev/mingtao/T5_longevity.py
Library	         keywords/T5Platform.py
Library	         keywords/T5ZTN.py 
Library	         keywords/T5Utilities.py 
Library	         keywords/T5.py
Library          keywords/T5L3.py
Library	         keywords/Controller.py
Library	         keywords/SwitchLight.py
Resource		 testsuites_dev/mingtao/t5_upgrade_ztn_resource.txt 

# document: https://bigswitch.atlassian.net/wiki/display/BSC/T6+Upgrade section: 6.4
# The purpose of this test suite is to verify ztn function with upgrade.
# Test topology:  2 controllers,  2 spines and 6 leaves.
# Configuration:    
# Note: prerequisites:  ONIE installed switch 
#       no support: ONIE install and  ONIE (u-boot) upgrade. 
#                   loader upgrade  
#       support:  loader install, swi upgrade
#       upgrade commit/abort not implemented
# In this Test Suite the following terms will be used:
# - stand-by switch - switch configured on the controller (with MAC address) with no fabric-role specified
# - provisioned switch - switch configured on the controller (with MAC address) with fabric role, forwarding traffic
# - suspended switch - switch connected to fabric but not configured on the controller,
#   or running wrong version of Swith Light, or with incorrect running-config
#
# Typical upgrade check:  configuration  - remain
#                         log files     -  not wiped out
#                         traffic       -  loss based on the test case 
#                         fabric config -  remain
#                         forwarding table - converged if switch need to reload
# 

* Variable
${short}  1
${medium}  10
${long}   30
${verylong}    120 
${upgradetime}    600 
${image}    bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/lastSuccessful/archive/controller-upgrade-*-SNAPSHOT.pkg
${image1}    bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/lastSuccessful/archive/controller-upgrade-*-SNAPSHOT.pkg
${image2}    bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/lastSuccessful/archive/controller-upgrade-*-SNAPSHOT.pkg
${image3}    bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/lastSuccessful/archive/controller-upgrade-*-SNAPSHOT.pkg 

${config}   scp://root@10.8.28.6:/home/mingtao/config_basic
${h2ip}     20.20.20.24
${h1ip}  	20.20.20.25
${File}     script.log
 
* Test Case
 
###### single node controller upgrade #####################


UPGRADE: T1.1 Verify upgrade (controller - upgrade, switch - upgrade ) 
	${result}=   Verify single node upgrade    ${image1}
	Run Keyword if   '${result}' != 'True'    Fatal Error    stop all the remain testcases
	
	[Tags] 	sanity  upgrade_ztn     


UPGRADE: T1.2 Verify upgrade(controller - upgrade, switch image/config - no need) 
	Run Keyword if   '${image1}' != '${image2}'      Verify single node upgrade    ${image2} 	 	
 
  		
	[Tags] 	sanity  upgrade_ztn    

UPGRADE: T1.3 Verify upgrade for same version(same image) 
	Run Keyword if   '${image1}' != '${image3}'		Verify single node upgrade    ${image3}  		
	[Tags] 	sanity  upgrade_ztn    

    

UPGRADE: T1.5 upgrade launch suspend
	Verify single node upgrade    ${image}    suspend    		
	[Tags] 	sanity  upgrade_ztn    
   
UPGRADE: T1.6 upgrade launch timeout
	Verify single node upgrade    ${image}    switch-timeout 900   		
	[Tags] 	sanity  upgrade_ztn    

    

UPGRADE: T1.8 upgrade launch suspend timeout
	Verify single node upgrade    ${image}    suspend switch-timeout 900   		
	[Tags] 	sanity  upgrade_ztn     
   

UPGRADE: T1.9 Verify upgrade rollback  
	log  step 0 - snap shot current state
  		${key_a}=   bash_get_key 	 
		${c1config_before}=   cli_take_snapshot   c1  run_config=yes			 
		Verify Ping   h1     ${h2ip}		
 		fabric_integrity_checker    before   single		
		bash ping background start    h1   label=upgrade_ping   dest_ip=${h2ip}

	log  step 1 - rollback
		cli   master    show boot partition			
		${result}= 	cli_boot_partition  
 		Should be True     ${result} 
		sleep  ${verylong} 	
		cli   master    show switch		
  		 
 	log  step 2 - check switch are upgrade with new image, and configuration is right.
		Reboot all the suspended switch	
		sleep  ${long}
		Verify switch are booted with correct image			
 	
  		
 	log  step 3 - check system ssh key is not changed and traffic can forward
 	 	 
 		${key_b}=   bash_get_key   
 		Should Be Equal as Strings  ${key_a}  ${key_b} 
 		${c1config_after}=   cli_take_snapshot   c1  run_config=yes
		Should Be Equal    ${c1config_before}    ${c1config_after} 	
		
 		# stop the ping and see how much traffic lost
		${result}=  bash ping background stop    h1   label=upgrade_ping 
		log  there are ${result} ping packet loss, 1 ping per sec
		Log to file     ${File}    there are ${result} ping packet loss, 1 ping per sec
		Log To Console   there are ${result} ping packet loss, 1 ping per sec
 		fabric_integrity_checker    after   single   		
		Verify Ping   h1     ${h2ip} 				
 			  		   		
	[Tags] 	sanity  upgrade_ztn    


UPGRADE: T1.10 standby switch behavior during upgrade 
	log  step 0 - move switch to standby
		Move switch from provisioned to standby     s8
		cli  master   show running-config switch 
		cli  master   show switch 
 				
 	log  step 1 - upgrade controller
		Copy image   ${image}
		Stage image 
		Launch image 
		
	log  step 2 - verify s8 still in suspended state
		sleep  ${verylong}
  		Wait Until Keyword Succeeds  	10 min	30 sec   Verify all switches connected back 
 		${alias}=   get_node_alias   s8 	
 		cli  master    show switch	
  		${switches}=  rest_get_suspended_switch 
 		Should Contain     ${switches}    ${alias}    						 
		${reason}=  rest_get_switch_suspended_reason   c1    ${alias}
		Should Be Equal   	No fabric role configured     ${reason}		 

	log  step 3 - add the switch back and reboot
		Add switch as provisioned switch	s8
		Reboot all the suspended switch	
		Verify switch are booted with correct image	
						      	      
	[Tags] 	full   upgrade_ztn     run

UPGRADE: T1.11 disconnected switch behavior during upgrade 
	log  step 0 - config an no existing switch				
 	log  step 1 - upgrade controller
	Manual passed
 		      	      
	[Tags] 	full   upgrade_ztn   manual 
 

 
SYNC: T2.1 Verify switch config in controller is pushed to swith(event trigger reload) 
	log  step 1 - modify switch related config in controller(NTP, Logging, SNMP)
   		Modify ZTN related config (NTP,SNMP,LOGGING) 
   		sleep  ${long} 	
	log  step 2 - verify switch startup config is changes for controller
		Verify All Switch Running Config  
		 
	log  step 3 - verify switch running config all updated
 		Verify All Switch Startup Config
 		
	log  step 4 - verify switch running config all updated to default
		Default ZTN related config (NTP,SNMP,LOGGING)	 
		Verify All Switch Running Config 
 		Verify All Switch Startup Config				
	[Tags] 	sanity    upgrade_ztn  


SYNC: T2.2 Verify switch config in controller is pushed to switch(through cli reload) 
	log  step 1 - modify switch related config in switvh(NTP, Logging, SNMP)
#		console_switch_config   s3   snmp-server contact try			 
	log  step 2 - perform cli reload		 
	log  step 3 - verify switch running config all revert back
	log  need to wait couple sec after zerotouch leaf0-a config-push
	Manual passed	       
	[Tags]  full    manual   upgrade_ztn    

SYNC: T2.3 Verify controller reboot with config changes in switch
	log  step 1 - modify switch related config in controller(NTP, Logging, SNMP) 
	log  step 2 - perform controller reboot  		
	log  step 3 - verify switch startup config and running config all updated
	Manual passed	 	 
		
	[Tags] 	full    manual   upgrade_ztn  
 

EVENT: T3.1 controller reboot 
	log  step 0 - check all switches are connected
		cli  master    show switch
		${switches}=  rest_get_suspended_switch 
		Should Be Empty   	${switches}
	log  step 1 - reboot controller
		cluster_node_reboot	c1 	
		sleep  ${long}		
   	log  step 2 - switch connected back with correct config
		Verify switch are booted with correct image	
				
   	log  step 3 - no traffic loss
		Verify Ping   h1     ${h2ip}		
 		
	[Tags] 	full   upgrade_ztn 

EVENT: T3.2 switch reboot options (switch should use image cash locally)
	log  step 0 - check all switches are connected
		cli    master  show switch
		${switches}=  rest_get_suspended_switch 
		Should Be Empty   	${switches}
		
 	log  step 1 - reboot switches by ip and very switch connect back with correct image and config
		Reboot all switches by ip 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 
		
 	log  step 2 - reboot switches by name and very switch connect back with correct image and config		
 		Reboot all switches by alias 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 3 - reboot switches by mac and very switch connect back with correct image and config			
		Reboot all switches by mac
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 4 - reboot switches all and very switch connect back with correct image and config			
		cli_reboot_switch_all 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 5 - reboot switches left/right and very switch connect back with correct image and config
  	   
   	log  step 6 - check traffic loss
 		Verify Ping   h1     ${h2ip}	
 		      
	[Tags] 	sanity     upgrade_ztn 
	
EVENT: T3.3 switch disconnected/connected during controller upgrade
	log  step 1 - check all switches are connected
		cli   master  show switch
		${switches}=  rest_get_suspended_switch 
		Should Be Empty   	${switches}
   	log  step 2 - switch disconnected during controller image copy   	
   	log  step 3 - switch connected back after image copy 
   	log  step 4 - switch disconnected during controller image stage
   	log  step 5 - switch connected back after image stage
   	log  step 6 - switch disconnected during controller image launch
   	log  step 7 - switch connected back after image launch 
   	log  step 6 - switch disconnected during controller reboot
   	log  step 7 - switch connected back after controller come back
  	      
	[Tags] 	full  manual-untested   upgrade_ztn 
	
	

EVENT: T3.6 new switch join as provisioned switch after upgrade (1 - swi upgrade, 1 - loader install)
	log  step 0 - prepare 2 switches, 1 need swi upgrade, 1 is new box
#		Remove switch from controller   s7
#		Remove switch from controller   s8
#		# s8 delete all the config
#		telnet_reset_switch_to_factory_default   s8 
		
    log  step 1 - upgrade controller  
# 		Verify single node upgrade    ${image} 
 		
 	log  step 2 - upgrade controller 			      	
#		Add switch as provisioned switch   s7
#		Add switch as provisioned switch   s8
				
 	log  step 3 - switch join as provisioned switch
#		Verify switch are booted with correct image
#		Verify All Switch Startup Config 
#		Verify All Switch Running Config 
	Manual passed       	      
	[Tags] 	full   upgrade_ztn    manual


EVENT: T3.7 new switch join as standby switch after upgrade (1 - swi upgrade, 1 - loader install)
	log  step 0 - prepare 2 switches, 1 need swi upgrade, 1 is new box
#		Remove switch from controller   s7
#		Remove switch from controller   s8
#		# s8 delete all the config
#		telnet_reset_switch_to_factory_default   s8 
		
    log  step 1 - upgrade controller  
# 		Verify single node upgrade    ${image} 
 		
 	log  step 2 - upgrade controller 			      		 	      	
#		Add switch as standby switch   s7
#		Add switch as standby switch   s8
     log  step 3 - switch show stay in suspended   
#		sleep  ${verylong}   	
#		${alias}=   get_node_alias   s7	
#		${reason}=  rest_get_switch_suspended_reason   c1    ${alias}
#		Should Be Equal   	No fabric role configured     ${reason}
#		${alias}=   get_node_alias   s8	
#		${reason}=  rest_get_switch_suspended_reason   c1    ${alias}
#		Should Be Equal   	No fabric role configured     ${reason}
	Manual passed 	 
	  
	[Tags]  full   upgrade_ztn    manual 

 	
EVENT: T3.8 new switch join as suspended switch (1 - swi upgrade, 1 - loader install)
	log  step 1 - upgrade controller
    log  step 2 - switch join during/after controller image launch
	Manual passed	 
   	      
	[Tags] 	sanity  manual  upgrade_ztn 
 
 
 