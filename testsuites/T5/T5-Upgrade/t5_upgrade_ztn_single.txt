* Settings
Documentation    T5 upgrade Test Suite
Suite Setup      Upgrade Suite Setup single controller
Suite Teardown   Upgrade suite teardown
Test Setup       Upgrade base test setup
Test Teardown    Upgrade base test teardown
Force Tags       T5   Ironhorse   single_node   upgrade
Library			 OperatingSystem
Library          keywords/BsnCommon.py
Library			 keywords/Mininet.py
Library		     keywords/Host.py
Library		     keywords/Ixia.py
Library		     keywords_dev/mingtao/T5_longevity.py
Library	         keywords/T5Platform.py
Library	         keywords/T5ZTN.py 
Library	         keywords/T5Utilities.py 
Library	         keywords/T5.py
Library          keywords/T5L3.py
Library	         keywords/Controller.py
Library	         keywords/SwitchLight.py
Resource		 testsuites_dev/mingtao/t5_upgrade_ztn_resource.txt 

# document: https://bigswitch.atlassian.net/wiki/display/BSC/T6+Upgrade section: 6.4
# The purpose of this test suite is to verify ztn function with upgrade.
# Test topology:  2 controllers,  2 spines and 6 leaves.
# Configuration:    
# Note: prerequisites:  ONIE installed switch 
#       no support: ONIE install and  ONIE (u-boot) upgrade. 
#                   loader upgrade  
#       support:  loader install, swi upgrade
#       upgrade commit/abort not implemented
# In this Test Suite the following terms will be used:
# - stand-by switch - switch configured on the controller (with MAC address) with no fabric-role specified
# - provisioned switch - switch configured on the controller (with MAC address) with fabric role, forwarding traffic
# - suspended switch - switch connected to fabric but not configured on the controller,
#   or running wrong version of Swith Light, or with incorrect running-config
#
# Typical upgrade check:  configuration  - remain
#                         log files     -  not wiped out
#                         traffic       -  loss based on the test case 
#                         fabric config -  remain
#                         forwarding table - converged if switch need to reload
# 

* Variable
${short}  1
${medium}  10
${long}   30
${verylong}    120 
${upgradetime}    600 
#${image}     bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/builds/3545/archive/*.pkg
${image}      bsn@jenkins:/var/lib/jenkins/jobs/bcf-2.0.1/lastSuccessful/archive/controller-*.pkg
${image1}     bsn@jenkins:/var/lib/jenkins/jobs/bcf-2.0.1/lastSuccessful/archive/controller-*.pkg
${image2}     bsn@jenkins:/var/lib/jenkins/jobs/bcf-2.0.1/lastSuccessful/archive/controller-*.pkg  

${qcowimage}   bsn@jenkins:/var/lib/jenkins/jobs/bcf_master/lastSuccessful/archive/controller-bcf-*.qcow2
${bigtapimage}   bsn@10.8.28.6:/home/bsn/controller-upgrade-bigtap-3.0.0-SNAPSHOT.pkg

${config}   scp://root@10.8.28.6:/home/mingtao/config_basic
${h2ip}     20.20.20.24
${h1ip}  	20.20.20.25
${File}     script.log
${vip}      10.8.28.20
 
 
* Test Case
 
###### single node controller upgrade #####################


UPGRADE: T1.1 Verify upgrade (controller - upgrade, switch - upgrade ) 
	${result}=   Verify single node upgrade    ${image1}
	Run Keyword if   '${result}' != 'True'    Fatal Error    stop all the remain testcases
	
	[Tags] 	sanity  feature  upgrade_ztn     run


UPGRADE: T1.2 Verify upgrade(controller - upgrade, switch image/config - no need) 
	Run Keyword if   '${image1}' != '${image2}'      Verify single node upgrade    ${image2} 	 	 
  		
	[Tags] 	sanity  feature  upgrade_ztn    

UPGRADE: T1.3 Verify upgrade for same version(same image) 
	${result}=   Verify single node upgrade    ${image2}  
	Run Keyword if   '${result}' != 'True'    Fatal Error    stop all the remain testcases
			
	[Tags] 	sanity  feature  upgrade_ztn    
    

UPGRADE: T1.4 upgrade launch timeout
	Verify single node upgrade    ${image}    switch-timeout 900   		
	[Tags] 	sanity  feature  upgrade_ztn      
     

UPGRADE: T1.5 Verify upgrade rollback  
	log  step 0 - snap shot current state
  		${key_a}=   bash_get_key 	 
		${c1config_before}=   cli_take_snapshot   c1  run_config=yes			 
		Verify Ping is successful   h1     ${h2ip}		
 		fabric_integrity_checker    before   single		
		bash ping background start    h1   label=upgrade_ping   dest_ip=${h2ip}

	log  step 1 - rollback
		cli   master    show boot partition			
		${result}= 	cli_boot_partition  
 		Should be True     ${result} 
		sleep  ${verylong} 	
		cli   master    show switch		
  		 
 	log  step 2 - check switch are upgrade with new image, and configuration is right.
		Reboot all the suspended switch	
		sleep  ${long}
		Verify switch are booted with correct image			
 	
  		
 	log  step 3 - check system ssh key is not changed and traffic can forward
 	 	 
 		${key_b}=   bash_get_key   
 		Should Be Equal as Strings  ${key_a}  ${key_b} 
 		${c1config_after}=   cli_take_snapshot   c1  run_config=yes
		Should Be Equal    ${c1config_before}    ${c1config_after} 	

		Verify Ping is successful    h1     ${h2ip}
 		# stop the ping and see how much traffic lost
		${result}=  bash ping background stop    h1   label=upgrade_ping 
		log  there are ${result} ping packet loss, 1 ping per sec
		Log to file     ${File}    there are ${result} ping packet loss, 1 ping per sec
		Log To Console   there are ${result} ping packet loss, 1 ping per sec
 		fabric_integrity_checker    after   single   		
  			  		   		
	[Tags] 	sanity  feature  upgrade_ztn    


UPGRADE: T1.6 standby switch behavior during upgrade 
	log  step 0 - move switch to standby
		Move switch from provisioned to standby     s8
		enable  master   show running-config switch 
		enable  master   show switch 
 				
 	log  step 1 - upgrade controller
 		Copy image if no image exist   c1
	 
		Stage image 
		${result}=     cli_upgrade_launch    node=c1   soft_error=True
		Should Contain  ${result}     suspended switch		
 
	log  step 2 - add the switch back and reboot
		Add switch as provisioned switch	s8
		cli  master   show running-config switch 
		cli  master   show switch 
		sleep  5
		Verify all switches connected back
 						      	      
	[Tags] 	full   feature  upgrade_ztn     

UPGRADE: T1.7 disconnected switch behavior during upgrade 
	log  step 0 - config an no existing switch
		config   master  switch dummy	
		config   master  mac 70:72:cf:00:00:00 
					
 	log  step 1 - upgrade controller
 		Copy image if no image exist    c1
		Stage image 
		Launch image
		sleep  ${verylong} 	
		
	log  step 2 - dummy is the only disconnected switch
 		${switches}=  rest_get_disconnect_switch
		log   the disconnected switches are ${switches}  	
		Should Contain   ${switches}	dummy  
		 
	log  step 3 - remove dummy, all switches are in connected state		
		config   master  no switch dummy			 
		sleep  5
		Verify all switches connected back
 		      	      
	[Tags] 	full   feature  upgrade_ztn    run
 
UPGRADE: T1.8 suspended switch behavior during upgrade 
	log  step 0 - prepaare suspended switch				
 	log  step 1 - upgrade controller
 		 
 		      	      
	[Tags] 	full   feature  upgrade_ztn   manual    skipped


UPGRADE: T1.9 upgrade with static ZTN server
	log  step 1 - get the ip address of two controller
		${c1ip}=  get_node_ip  c1 
   	log  step 2 - add ztn servers to the boot-config
	 	${switches}=  get_all_switch_nodes  
		log   switches are ${switches}
 		: FOR    ${sw}   IN   @{switches}   
 		\   console_bash_switch_add_ztnserver    ${sw}     ${c1ip}
 		Wait Until Keyword Succeeds  	10 min	30 sec   Verify all switches connected back	
  		
 	log  step 3 - upgrade the controllers
		Verify single node upgrade    ${image}
		
	log  step 4 - remove the static ztn server
 		: FOR    ${sw}   IN   @{switches}
		\  console_bash_switch_default_boot_config    ${sw} 
		cli_reboot_switch_all  
		sleep  60
		Wait Until Keyword Succeeds  	10 min	30 sec   Verify all switches connected back	
		
	[Tags] 	fully   feature  upgrade_ztn     run

UPGRADE: T1.10 upgrade with not ZTN server configured
	log   covered in test case T1.1
	
	[Tags] 	fully   feature  upgrade_ztn   
 
UPGRADE: T1.11 VIP in upgrade
	T5Platform.rest_configure_virtual_ip   ${vip}
 	${result}=   cli_show_virtual_ip
 	Should Be Equal As Strings  ${result}  ${vip}
	
	Verify single node upgrade    ${image}
 	
 	${result}=   cli_show_virtual_ip
 	Should Be Equal As Strings  ${result}  ${vip}
 	T5Platform.rest_delete_virtual_ip
 	
  	[Tags] 	full    feature  upgrade_ztn    


UPGRADE: T1.12 After the upgrade verify the configuration integrity 
	log   covered in T1.1   
    
  	[Tags] 	full    feature  upgrade_ztn       


UPGRADE: T1.13 Verify whether the saved configs are retained after the upgrade
	log   covered in T1.1  
   
  	[Tags] 	full    feature  upgrade_ztn      
 
 
SYNC: T2.1 Verify switch config in controller is pushed to swith(event trigger reload) 
	log  step 1 - modify switch related config in controller(NTP, Logging, SNMP)
   		Modify ZTN related config (NTP,SNMP,LOGGING) 
   		sleep  ${long} 	
	log  step 2 - verify switch startup config is changes for controller
		Verify All Switch Running Config  
		 
	log  step 3 - verify switch running config all updated
 		Verify All Switch Startup Config
 		
	log  step 4 - verify switch running config all updated to default
		Default ZTN related config (NTP,SNMP,LOGGING)
   		sleep  ${long} 		 			 
		Verify All Switch Running Config 
 		Verify All Switch Startup Config				
	[Tags] 	sanity    feature  upgrade_ztn   


SYNC: T2.2 Verify switch config in controller is pushed to switch(through cli reload) 
	log  step 1 - modify switch related config in switch(NTP, Logging, SNMP)
		telnet_run		s1     enable;configure
		telnet_run		s1     ntp server 0.us.pool.ntp.org
		telnet_run		s1     exit;exit   
		telnet_run		s1     show running-config 	
		${sw}=  get_node_alias    s1
	log  step 2 - perform cli reload	
		enable   master    system config-push switch ${sw}  	 
	log  step 3 - verify switch running config all revert back
		Verify All Switch Running Config	
		
		telnet_run		s1     show running-config 	
		       
	[Tags]  full   feature  upgrade_ztn       

SYNC: T2.3 Verify controller reboot with config changes in switch
	log  step 1 - modify switch related config in controller(NTP, Logging, SNMP) 
		telnet_run		s1     enable;configure
		telnet_run		s1     ntp server 0.us.pool.ntp.org
		telnet_run		s1     exit;exit   
		telnet_run		s1     show running-config 	
		${sw}=  get_node_alias    s1
	
	log  step 2 - perform controller reboot  	
 		cluster_node_reboot   
		sleep  ${long}				
	log  step 3 - verify switch startup config and running config all updated
		Verify All Switch Running Config 
 		Verify All Switch Startup Config					  	 
		
	[Tags] 	full   feature  upgrade_ztn       
 

EVENT: T3.1 controller reboot after upgrade
	log  step 0 - upgrade controllers
 		Verify single node upgrade    ${image}		

	log  step 1 - reboot controller
		cluster_node_reboot	  	
		sleep  ${long}		
   	log  step 2 - switch connected back with correct config
		Verify switch are booted with correct image	
  		
	[Tags] 	full   feature  upgrade_ztn   


	
EVENT: T3.2 switch disconnected/connected during controller upgrade
	log  step 1 - check all switches are connected
    log  step 2 - switch disconnected during controller image copy   	
   	log  step 3 - switch connected back after image copy 
   	log  step 4 - switch disconnected during controller image stage
   	log  step 5 - switch connected back after image stage
   	log  step 6 - switch disconnected during controller image launch
   	log  step 7 - switch connected back after image launch 
   	log  step 6 - switch disconnected during controller reboot
   	log  step 7 - switch connected back after controller come back
  	      
	[Tags] 	full   feature  upgrade_ztn   manual    skipped
	
	

EVENT: T3.3 new switch join as provisioned switch after upgrade (1 - swi upgrade, 1 - loader install)
	log  step 0 - prepare the switches
		Set switch to factory default for next reboot	  s8
		Remove switch from controller		s7
		Remove switch from controller		s8
		power_cycle_switch  s7   
	    power_cycle_switch  s8   
		 
	log  step 1 - upgrade controller
		Verify single node upgrade    ${image}
	
	log  setp 2 - connect new switch as provisioned switch
		 			
		Add switch as provisioned switch   s7
		Add switch as provisioned switch   s8
				
		Verify switch are booted with correct image
 	[Tags] 	full   feature  upgrade_ztn    


EVENT: T3.4 new switch join as standby switch after upgrade (1 - swi upgrade, 1 - loader install)
	log  step 0 - prepare the switches
		Set switch to factory default for next reboot	  s8
		Remove switch from controller		s7
		Remove switch from controller		s8
		power_cycle_switch  s7   
	    power_cycle_switch  s8    
		 
	log  step 1 - upgrade controller
	 	Verify single node upgrade    ${image}		
	log  setp 2 - connect new switch as standby switch
		Add switch as standby switch   s7
		Add switch as standby switch   s8
		Wait Until Keyword Succeeds  	10 min	30 sec   Verify all switches connected back 
		Add switch as provisioned switch   s7
		Add switch as provisioned switch   s8
		Verify switch are booted with correct image

 	  
	[Tags]  full   feature  upgrade_ztn     

MIS: T10 switch reboot options (switch should use image cash locally)
	log  step 0 - check all switches are connected
		cli    master  show switch
		${switches}=  rest_get_suspended_switch 
		Should Be Empty   	${switches}
		
 	log  step 1 - reboot switches by ip and very switch connect back with correct image and config
		Reboot all switches by ip 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 
		
 	log  step 2 - reboot switches by name and very switch connect back with correct image and config		
 		Reboot all switches by alias 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 3 - reboot switches by mac and very switch connect back with correct image and config			
		Reboot all switches by mac
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 4 - reboot switches all and very switch connect back with correct image and config			
		cli_reboot_switch_all 
		sleep  ${long}
		Verify switch are booted with correct image
		Verify All Switch Startup Config 
		Verify All Switch Running Config 

 	log  step 5 - reboot switches left/right(not there anymore)
  	   
   	log  step 6 - check traffic loss
		Verify Ping is successful    h1     ${h2ip}  	
 		      
	[Tags] 	sanity     feature  upgrade_ztn      
 