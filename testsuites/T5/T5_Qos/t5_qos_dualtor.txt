* Setting
Documentation   T5 QoS two spine three rack with dual-tor Test Suite
Suite Setup     setup topology
Suite Teardown  teardown topology
#Suite Setup     base suite setup
#Suite Teardown  base suite teardown
Test Setup       base test setup
Test Teardown	 base test teardown
#Test Setup      setup topology
#Test Teardown   teardown topology
#Suite Setup		setup topology
#Suite Teardown	teardown topology
Force Tags      T5 Sanity
Library         keywords/BsnCommon.py
Library         keywords/Mininet.py
Library         keywords/T5.py
Library         keywords/T5L3.py
Library         keywords_dev/animesh/BsnCommonShow.py
Library         OperatingSystem
Library         Collections
Library         Dialogs

## Test Tpology: https://docs.google.com/a/bigswitch.com/document/d/1xxI_9UhNAFUwAIZ_lDHNO-P0r4Xaomxfwv2UDsLuRJo/edit
## FS/DS doc: https://bigswitch.atlassian.net/wiki/display/BSC/QoS+Implementation+on+Switch
##Notes:
##All Hosts are assumed to be Ixia ports, Total 5 ixia ports needed. one is destination and 4 are source ports

##Traffic pattern
## 4 Source ports sending to one destination port
## traffic between tors in a dual-tor (peer-link?)
## traffic between spine and leaf



##Controller CLI options:
## QoS enabled by default
## Mapping/Weights are below
##For data plane: .1p
## priority 0, 1 -> queue 0 Weight 2
## priority 2, 3 -> queue 1 Weight 4
## priority 4, 5 -> queue 2 Weight 6
## priority 6, 7 -> queue 3 Weight 8
 
 #Control traffic Checck with DEV--->
 ##The control plane packets -> queue 8
# CPU port has 7 queues going to 3 RX channels
#chan 1 <- queue 0 with weight 50, Fabric LLDP/LACP
#chan 1 <- queue 1 with weight 20, Front Panel LLDP/LACP
#chan 2 <- queue 2 with weight 15, APR/DHCP
#chan 2 <- queue 3 with weight 15, L2SrcMiss/StationMove
#chan 3 <- queue 4 with weight 5, Debug/Stat
#chan 3 <- queue 5 with weight 5, L3ToCPU
#chan 3 <- queue 6 with weight 5, L3DstMiss
##Data plane weight 2, Control plane weight 20
 
 
 ## TESTPLAN divided into below sections
 #1.) CLI
 #2.) Funcitonality/HW verifications (Control, data and ratelimiters at HW chip, and at Controller)
 #3.) Interactions with other Network infra elements (HA, add other items)
 #4.) Few negitive cases (process restarts)
 #5.) Few Scale cases               
                           
#
#GLobal Verifications
## Any Exceptions in Conttoller, LLDP/LACP timeouts? Packins
## ofad crfashes, if any

## Default QoS: all weights are equal share, however .1p mappings are honored as above


* Test Cases
##***********
##CLI Related test cases

	
#FUNCTIONALITY:
##ONLY DATA TRAFFIC NO CONTROL TRAFFIC
T1 With default QoS verify flow mappings to queues
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues

T2 With default QoS verify Weights
	[Tags]
	Default QoS
	Start traffic
	Verify flow weights on egress pipe

T3 with default QoS local leaf traffic
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe
	
T4 with default QoS leaf to leaf traffic in a rack  #Data pkt flooding ratelimiters --->
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T5 With default QoS leaf to Spine traffic
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T6 With default QoS leaf to leaf traffic with 90% low priority and 10% High priority
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T7 With default QoS leaf to spine traffic with 90% low priority and 10% High priority
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T8 Mix T6 and T7

T9 With default QoS leaf to leaf start each priority traffic one by one #same interface sending and receiving --->
	[Tags]
	Default QoS
	Start low priority traffic 100%
	Start second low priority traffic 100%
	Start third low priority traffic 100%
	Start High priority traffic 100%
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T10 Repeat T9 with leaf to spine

T11 Mix T10 and T9

T12 Repeat T1 to T11 with frame sizes starting from 64 to 9216

T13 Mix and match flows with random frame sizes with multiple flows

T14 Repeat above all with two spines and 4 leafs, each source is from different leaf sending to one destination leaf

## WITH CONTROL TRAFFIC 
T15 Repeat above all with CONTROL TRAFFIC (from T1 to T14)
	Verify the rate limiters on HW and Controller

## WITH QoS
##ONLY DATA TRAFFIC NO CONTROL TRAFFIC
T16 With QoS verify flow mappings to queues
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues

T17 With QoS verify Weights
	[Tags]
	Default QoS
	Start traffic
	Verify flow weights on egress pipe

T18 with QoS local leaf traffic
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe
	
T19 with QoS leaf to leaf traffic in a rack
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T20 With QoS leaf to Spine traffic
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T21 With QoS leaf to leaf traffic with 90% low priority and 10% High priority
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T22 With QoS leaf to spine traffic with 90% low priority and 10% High priority
	[Tags]
	Default QoS
	Start traffic
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T23 Mix T22 and T21

T24 With QoS leaf to leaf start each priority traffic one by one
	[Tags]
	Default QoS
	Start low priority traffic 100%
	Start second low priority traffic 100%
	Start third low priority traffic 100%
	Start High priority traffic 100%
	Verify flow mappings to HW queues
	Verify flow weights on egress pipe

T25 Repeat T9 with leaf to spine

T26 Mix T25 and T24

T27 Repeat T16 to T26 with frame sizes starting from 64 to 9216

T28 Mix and match flows with random frame sizes with multiple flows

T29 Repeat above all with two spines and 4 leafs, each source is from different leaf sending to one destination leaf

## WITH CONTROL TRAFFIC 
T30 Repeat above all with CONTROL TRAFFIC (from T1 to T14)
	Verify control rate limiters on HW and Controller
	
T31 shut no shut switch interface

T32 Remove Add tenant interface

T33 Remove Add vns interface

T34 Remove Add switch
#Add a new switch
#counters. .1p data pkt counters and control pkts

T35 HA failover
T36 Reboot standby
T37 Reboot master
T38 poweroff switch
T39 poweron switch
T40 poweroff master controller
T41 poweroff slave controller

T42 Shut Mgmt port in switch and connect back
T43 Verify syslogs for any scale errors
T44 restart ofad
T45 restart floodlight

	
