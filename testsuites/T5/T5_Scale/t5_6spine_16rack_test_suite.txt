== T5 Scale Test Suite ==

* Setting
Documentation    T5 Scale Test Suite
Suite Setup      base suite setup
Suite Teardown   base suite teardown
#Test Setup       T5 base test setup
#Test Teardown    T5 base test teardown
Force Tags      IronHorse  T5
Library          keywords/BsnCommon.py
Library          keywords/T5.py
#Library	         keywords_dev/svoora/T5Fabric.py
Library			 keywords_dev/svoora/T5_Scale.py
Library		 	 keywords/T5Platform.py
Library			 keywords/T5L3.py
Library          keywords/Mininet.py


#Documentation   T5 Scale - Iron Horse
#...             export BIGROBOT_SUITE=t5_6spine_16rack_test_suite.txt
#...             Section A: Build a Scale configuration file
#...             Section B: Configure master/slave controllers in one cluster
#				Section C: Copy Scale Configuration file to Controller master
#					Topology:
#						6 spines, 16 rack (dual leaf)
#						All leafs have 40G uplinks to spines
#						Ixia and big-tap be used for traffic testing
#					Scale Config: 
#						1022/1023 user Tenants? (+ 1 system tenant vrf-0 + 1 external vrf-1 ? = 1024)
#						4088/4092 user VNS (4 VNS/ tenant, 0x001 and 0xfff  is reserved, 0x000 is not allowed =  4091/4095. Each Ixia ports will have 4k vlan configured)	
#						4088/4092 IP subnets (one per VNS subject to above limits)
#						All VNS will be applied across all leaf nodes including service rack
#						48K L3 host  for intra-tenant routing
#						48K L2 host 
#						16K CIDR routes ? (tables might be used for inter-tenant routing)
#						3K policy rules per leaf switches
#						ARP 2k per sec per switch
#					Performance:
#						Controller PACKET-IN 100k per second
#						No DHCP relay configured. No overlapping subnet.
#						MAC learning rate 2000 per sec for controller, and 200 per switch?
#					Gneralnote:
#						All testcases will be run with L2/L3 and external traffic
#						Will monitor and take snapshot of memory/cpu utilization on both controllers/switches
#						Monitor java exceptions
#						Traffic convergence time periods 


* Variables
${cfg_file_path}		/home/svoora/temp.txt
${cfg_file_server}		10.192.108.10
${file_server_passwd}	bsn
${copied_file}			scale_policy_cfg_2.txt
${monitor_file}			exception_file.log
${bm1_ip}				10.0.0.6
#${m_pg} 				leaf1-bm4
${m_pg}					leaf15-bm46

* Test Case 

copy config file to controller
	[Tags]  scaling
	Log To console  Getting config file server
	copy config from server  ${cfg_file_path}  ${cfg_file_server}  ${file_server_passwd}  ${copied_file} 
	verify file  ${copied_file}
	Log To Console  Verifiing cluster formation
	${returnVal}=  rest verify show cluster
	Should Be True  ${returnVal}
	
config controller with copied config file
	[Tags]  scaling   
	Log To console  Copying file to running-config
	#start monitoring the exception
	#${file_exist}=  verify file  ${copied_file}
	#Should Be True  ${file_exist}
	${return}=  cli copy file to running config  ${copied_file}	
	Should Be True  ${return}
	#sleep  60
	#stop monitoring the exception
	#parse monitoring file
	
verify config sync between controllers
	[Tags]  scaling   
	Log To Console  Verifying configuration between controllers
	${return}=  compare configuration
	Should Be True  ${return}
		
start mininet topology
	[Tags]  scaling   
	Log To Console  Starting mininet topology
	start monitoring the exception
	mininet start	
	sleep  60
	#pause               UNDER MININET!!!!
	verify fabric switch link and errors
	stop monitoring the exception
	parse monitoring file
	
ping bms to check L2 fwding
	[Tags]  scaling   	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log to console  Pinging from bm0 to bm5
    ${loss}=  mininet ping  bm0  bm5
    #pause                UNDER MININET!!!!
    Should Be True  ${loss} == 0
    Run Keyword Unless  ${loss} == 0    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
			
	
ping bms to check L3 fwding
	[Tags]  scaling   
	Log To Console  Removing Tenant1 and adding its vns to Tenant0
	rest delete tenant  Tenant1
	rest add vns ip  Tenant0  Tenant1-VNS1  10.10.2.1  24
	rest add vns  Tenant0  Tenant1-VNS1
	rest add portgroup to vns  Tenant0  Tenant1-VNS1  leaf0-bm1  -1
	rest add portgroup to vns  Tenant0  Tenant1-VNS1  ${m_pg}   -1
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	#Test pause execution (SHELL)
    #pause            Freeze!!!
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	
Remove spine0 and check L2/L3 
	[Tags]  scaling   
	Log To Console  Removing spine0 from configuation
	start monitoring the exception
	rest delete fabric switch  spine0 
	sleep  5
	stop monitoring the exception
	parse monitoring file
	
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	
Add spine0 back and check L2/L3
	[Tags]  scaling   
	Log To Console  Adding back spine0
	start monitoring the exception
	rest add switch  spine0
	rest add dpid  spine0  00:00:00:00:00:01:00:01
	rest add fabric role  spine0  spine		
	sleep  5
	stop monitoring the exception
	parse monitoring file
	
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	
stop and start mininet and verify L2/L3 fwding
	[Tags]  scaling     
	Log To Console  Stopping mininet in topo
	start monitoring the exception
	mininet stop	
	sleep  100
	#pause               UNDER MININET!!!!
	stop monitoring the exception
	parse monitoring file
	sleep  100
	Log To Console  Starting mininet in topo
	start monitoring the exception
	mininet start	
	sleep  100
	#pause               UNDER MININET!!!!
	stop monitoring the exception
	parse monitoring file
	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log to Console  Verifing Traffic
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	
	 
reboot master controller and verify L2/L3 fwding
	[Tags]  scaling   
	Log To Console  Rebooting master controller	 
	start monitoring the exception
	cli verify cluster master reboot
	sleep  20
	stop monitoring the exception
	parse monitoring file
	
	Log To console  Checking L2/L3 fwding
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	
reboot slave controller and verify L2/L3 fwding
	[Tags]  scaling   
	Log To Console  Rebooting slave controller	 
	start monitoring the exception
	cli verify cluster slave reboot
	sleep  20
	stop monitoring the exception
	parse monitoring file
	
	Log To console  Checking L2/L3 fwding
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100

cluster failover and verify L2/L3 fwding
	[Tags]  scaling   
	Log To Console  cluster failover	 
	start monitoring the exception
	rest verify cluster election take leader
	sleep  20
	stop monitoring the exception
	parse monitoring file
	
	Log To console  Checking L2/L3 fwding
	Log to console  Pinging from bm0 to 10.10.1.3
    ${loss}=  mininet ping  bm0  10.10.1.3
    Should Be True  ${loss} < 100
    Run Keyword Unless  ${loss} < 100    Collect Logs  ping failed so collecting all logs from Collect_Logs					 
	
	Log To console  Configuring mininet bms
	mininet host ipcfg  bm0  bm0-bond0  10.10.1.2  24
	mininet host ipcfg  bm5  bm5-bond0  10.10.1.3  24
	mininet host ipcfg  bm1  bm1-bond0  10.10.2.2  24
	mininet host ipcfg  bm4  bm4-bond0  10.10.2.3  24
	
	mininet host gw  bm0  10.10.1.1  bm0-bond0
	mininet host gw  bm5  10.10.1.1  bm5-bond0
	mininet host gw  bm1  10.10.2.1  bm1-bond0
	mininet host gw  bm4  10.10.2.1  bm4-bond0
	
	Log To console  Checking ping between vns
	${loss}=  mininet l3 ping  bm0  10.10.2.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm1  10.10.1.2
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm5  10.10.2.3
	Should Be True  ${loss} < 100
	${loss}=  mininet l3 ping  bm4  10.10.1.3
	Should Be True  ${loss} < 100
	


T1 Verify L2 Traffic Convergence with scale configuration first time
    Log    Build scale configuration with Scale Config
    Log    Copy Scale Config to Master Controller
    Log    Start Traffic on all ixia ports + BigTap ports
    Log    Copy scale config to running config
    Log    Measure traffic convergence time for L2 
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual passed
	

T2 Verify L3 Traffic Convergence with scale configuration first time
    Log    Build scale configuration with Scale Config
    Log    Copy Scale Config to Master Controller
    Log    Start Traffic on all ixia ports + BigTap ports
    Log    Copy scale config to running config
    Log    Measure traffic convergence time for L3
    #Manual untested
	[Tags] 	sanity    Manual  scaling
    Manual passed
     
T3 Verify L2/L3 Traffic Convergence with scale configuration first time
    Log    Build scale configuration with Scale Config
    Log    Copy Scale Config to Master Controller
    Log    Start Traffic on all ixia ports + BigTap ports
    Log    Copy scale config to running config
    Log    Measure traffic convergence time for L2/L3 mixed
    manual untested
	[Tags] 	sanity    Manual-untested   scaling

T4 Verify L2 traffic drop time after deleting the scale config
    Log    Repeat T1
    Log    Delete scale configuration under Master controller
    Log    Measure complete traffic drop time
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed

T5 Verify L3 traffic drop time after deleting the scale config
    Log    Repeat T2
    Log    Delete scale configuration under Master controller
    Log    Measure complete traffic drop time
    #Manual untested
	[Tags] 	sanity    Manual  scaling
    Manual failed
    
T6 Verify L2/L3 traffic drop time after deleting the scale config
    Log    Repeat T3
    Log    Delete scale configuration under Master controller
    Log    Measure complete traffic drop time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling


T7 Reboot entire fabric to measure time all flows start forwarding without any traffic drop
    Log    Copy scale configuration to master controller
    Log    Make sure fabric is in stable state
    Log    Start L2/L3 traffic profiles
    Log    Reboot entire fabric
    Log    Measure the time all traffic flows are forwarding without any packet drops
    Log    Repeat T7 for 32k/64k/96k/ L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
    Manual passed
    
T8 Generate ARP requests from all L3 hosts
    Log    Copy scale configuration to master controller
    Log    Make sure fabric is in stable state
    Log    Send ARP from all L3 hosts 2k ARPs per switch
    Log    Verify ARP resolve
    Log    Repeat T8 for 16k/32k/48k L3 hosts
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
    
    
T9 Controller HA fail-over via CLI
    Log    Repeat T3
    Log    Perform controller fail-over
    Log    Verify traffic loss
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T10 Controller HA fail-over/fail-back via CLI
    Log    Repeat T9
    Log    Make sure fabric in stable state
    Log    Perform controller fail-over/fail-back
    Log    Verify traffic loss
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T11 Controller HA fail-over/fail-back via CLI
    Log    Repeat T9
    Log    Fabric is not in stable state
    Log    Perform controller fail-over/fail-back
    Log    Verify traffic loss
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T12 Controller master reboot    
    Log    Repeat T9
    Log    Fabric is not in stable state
    Log    Master Controller reboot
    Log    Verify traffic loss
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T13 Controller slave reboot    
    Log    Repeat T9
    Log    Fabric is not in stable state
    Log    Master Controller reboot
    Log    Verify traffic loss
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	    
T14 Verify/Measure Non-Graceful operations    
    Log    Repeat T9
    Log    Fabric is not in stable state
    Log    Master Controller power off
    Log    Verify traffic loss/Standby take over
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	   

T15 Verify/Measure Non-Graceful operations    
    Log    Repeat T9
    Log    Fabric is not in stable state
    Log    Slave Controller power off
    Log    Verify traffic loss/Standby take over
    Log    Repeat for 32k/64k/96k L2/L3 hosts
    Manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	   
T16 Kill floodlight deamon in master contorller
    Log   Repeat T3
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   kill floodlight with kill option 9 under master controller
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    #manual untested
	[Tags] 	sanity    Manual  scaling
	manual failed
	
	
T17 Kill floodlight deamon in slave contorller
    Log   Repeat T3
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   kill floodlight with kill option 9 under slave controller
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    #manual untested
	[Tags] 	sanity    Manual  scaling
	manual passed
	
T18 Kill floodlight deamon in master/slave contorller
    Log   Repeat T3
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   kill floodlight with kill option 9 under master/slave controller
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T19 Power off one spine
    Log   Repeat T3    
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Power off one spine
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T20 Power on one spine    
    Log   Repeat T3    
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Power on one spine
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T21 Power off/on spine    
    Log   Repeat T19/T20 for multiple spines    
    Log   Make sure fabric is in stable state
    Log   Verify off/no traffic loss
    Log   Power on one spine
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T22 Power off one leaf
    Log   Repeat T3    
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Power off one leaf
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T23 Power on one leaf    
    Log   Repeat T3    
    Log   Make sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Power on one leaf
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T24 Power off/on leaf    
    Log   Repeat T23/T24 for multiple leaf racks    
    Log   Make sure fabric is in stable state
    Log   Verify off/no traffic loss
    Log   Power on one leaf
    Log   Verify fabric state
    Log   Measure traffic loss/Convergence time
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T25 Power down 16th rack
    Log   Repeat T3
    Log   Make sure fabric is in stable state
    Log   Verify traffic loss
    Log   Power down 16th rack (spines/leafs/servers)
    Log   Verify fabric stability
    Log   Measure traffic convergence/loss    
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	    
T26 Power up 16th rack
    Log   Repeat T25
    Log   Make sure fabric is in stable state
    Log   Verify traffic loss
    Log   Power up 16th rack (spines/leafs/servers)
    Log   Verify fabric stability
    Log   Measure traffic convergence/loss
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T27 Disconnect all switches from BSN controller
    Log   Repeat T3
    Log   Makre sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Disconnect all switch management ports
    Log   Verify controller stability
    Log   Measure time till traffic gets dropped
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T28 Reconnect all switches from BSN controller
    Log   Repeat T27
    Log   Reconnect all switch management ports
    Log   Verify controller stability
    Log   Measure time till traffic gets dropped
    manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	           
T29 Fail management switch one at a time               
    Log   Repeat T3
    Log   Makre sure fabric is in stable state
    Log   Verify no traffic loss
    Log   Fail one management switch 
    Log   Verify controller/fabric stability
    Log   Measure time till traffic gets dropped
    #Manual untested
	[Tags] 	sanity    Manual  scaling
	Manual failed
	
T30 Restart ofad in spine
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one spine
   Log   Verify/measure traffic loss/convergence
   #Manual untested
   [Tags] 	sanity    Manual  scaling
   Manual passed
	
T31 Restart ofad in leaf
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   #Manual untested
   [Tags] 	sanity    Manual  scaling
   Manual passed
	
T32 Verify L2 learning rate
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
T33 Verify L3 hosts ageout and relearn
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T34 Verify traffic loss/convergence after removing static endpoints
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T35 Verify image upgrade of controller
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   Manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T37 Verify parallel upgrades of all spines/leafs
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T38 Verify L2 mac move with multiple macs (repeat with 16k/32k/48k L2 hosts)
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T39 Verify L3 hosts move with inter-tenant traffic (repeat with 16k/32k/48k L3 hosts)
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual-untested
	[Tags] 	sanity    Manual  scaling
	
	
T40 Verify/measure traffic convergence/cli times for incremental config changes
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T41 Verify fabric stability on spine/leaf interface continuous flap
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T42 Verify traffic loss in headless mode of controller
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T43 Verify traffic loss in controller split brain mode
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T44 Verify traffic loss in half switches connected to master and rest have connection to standby controller
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T45 Repeat T44, all switches connect back to master, verify fabric stability
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T46 Verify snmp polling (get/getnext/getbulk)
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T47 Verify controller stability when switch sends line rate traffic to controller
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual-untested
	[Tags] 	sanity    Manual  scaling
	
	
T48 Verify switch stability when controller sends line rate traffic switch
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T49 Verify VIP changes
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	
T50 Verify policy changes
   Log   Repeat T3
   Log   Make sure fabric is in stable state
   Log   Verify no traffic loss
   Log   Restart ofad in one leaf
   Log   Verify/measure traffic loss/convergence
   manual untested
	[Tags] 	sanity    Manual-untested  scaling
	
	


	
	
* Keywords


start monitoring the exception
	Log To console  Starting to monitor Exception logs
	start monitor exception  ${monitor_file}
	${pid}=  pid return monitor file  master
	Should Be True  ${pid}
	${pid}=  pid return monitor file  slave
	Should Be True  ${pid}

stop monitoring the exception
	Log To Console  Stopping the monitor Exception logs
	${pid}=  pid return monitor file  master
	stop monitor exception  ${pid}  master
	${pid}=  pid return monitor file  slave
	stop monitor exception  ${pid}  slave
	
parse monitoring file
	Log To Console  Looking for exceptions in the file
	${size}=  parse exception  c1  ${monitor_file}
	#Should Be True  ${size} == 0
	${size}=  parse exception  c2  ${monitor_file}
	#Should Be True  ${size} == 0

verify fabric switch link and errors
	Log To Console  Verifing the fabric switch and links
	rest verify fabric switch all
	rest verify fabric link
	${result}=  rest verify fabric errors
	Should Be True  ${result} == 0
	
		

Collect Logs  [Arguments]  ${text}
    Log To Console  ${text}
	Dump Show Commands
 
